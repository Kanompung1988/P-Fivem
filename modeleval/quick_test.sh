#!/bin/bash

# Quick benchmark test with sample data
# à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸”à¸ªà¸­à¸šà¸£à¸°à¸šà¸š benchmark à¸­à¸¢à¹ˆà¸²à¸‡à¸£à¸§à¸”à¹€à¸£à¹‡à¸§

echo "======================================"
echo "ğŸ¯ Model Benchmark Quick Test"
echo "======================================"
echo ""

# Check if virtual environment is activated
if [[ -z "$VIRTUAL_ENV" ]]; then
    echo "âš ï¸  Virtual environment not activated"
    echo "   Run: source .venv/bin/activate"
    exit 1
fi

# Check if .env exists
if [ ! -f "../.env" ]; then
    echo "âš ï¸  .env file not found"
    echo "   Please create .env file with API keys"
    exit 1
fi

echo "ğŸ“‹ Running quick test with 5 test cases..."
echo "   Testing: Gemini-1.5-Flash (most cost-effective!)"
echo ""

# Run benchmark with Gemini-1.5-Flash (most cost-effective)
python benchmark_real_data.py --max-tests 5 --models Gemini-1.5-Flash

echo ""
echo "âœ… Quick test completed!"
echo ""
echo "ğŸ’¡ Gemini-1.5-Flash is the most cost-effective model ($0.07 per 1M tokens)"
echo ""
echo "ğŸ“Š To compare with other models:"
echo "   python benchmark_real_data.py --max-tests 20 --models Gemini-1.5-Flash GPT-4o-mini Typhoon-v2.5-30B"
echo ""
echo "ğŸ“Š To run full benchmark:"
echo "   python benchmark_real_data.py --max-tests 50"
echo ""
echo "ğŸ“„ To generate report:"
echo "   python report_generator.py results/benchmark_summary_*.json"
