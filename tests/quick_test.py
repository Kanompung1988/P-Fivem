#!/usr/bin/env python3
"""
Quick Model Test - à¸—à¸”à¸ªà¸­à¸šà¹€à¸£à¹‡à¸§à¹† à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸£à¸­à¸™à¸²à¸™
à¹€à¸¥à¸·à¸­à¸à¹à¸„à¹ˆà¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸ˆà¸²à¸à¹à¸•à¹ˆà¸¥à¸° category

Usage:
    python tests/quick_test.py
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))

load_dotenv()

# Quick test samples (2-3 per category)
QUICK_TEST_SAMPLES = [
    # Services & Pricing (2)
    {
        "question": "MTS PDRN à¸„à¸·à¸­à¸­à¸°à¹„à¸£à¸„à¸°",
        "category": "services_pricing",
        "expected": ["MTS", "PDRN", "à¸Ÿà¸·à¹‰à¸™à¸Ÿà¸¹à¸œà¸´à¸§"]
    },
    {
        "question": "MTS PDRN à¸£à¸²à¸„à¸²à¹€à¸—à¹ˆà¸²à¹„à¸«à¸£à¹ˆà¸„à¸°",
        "category": "services_pricing",
        "expected": ["à¸£à¸²à¸„à¸²", "à¸šà¸²à¸—"]
    },
    
    # Promotions (2)
    {
        "question": "à¸¡à¸µà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡à¸„à¸°",
        "category": "promotions",
        "expected": ["à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™"]
    },
    {
        "question": "Meso Promotion 5 Times 999 à¸„à¸·à¸­à¸­à¸°à¹„à¸£",
        "category": "promotions",
        "expected": ["Meso", "999", "5"]
    },
    
    # Clinic Info (2)
    {
        "question": "à¸„à¸¥à¸´à¸™à¸´à¸à¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¹„à¸«à¸™à¸„à¸°",
        "category": "clinic_info",
        "expected": ["à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆ", "à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ"]
    },
    {
        "question": "à¸ˆà¸­à¸‡à¸„à¸´à¸§à¹„à¸”à¹‰à¸¢à¸±à¸‡à¹„à¸‡",
        "category": "clinic_info",
        "expected": ["à¸ˆà¸­à¸‡", "à¸„à¸´à¸§"]
    },
    
    # Complex (2)
    {
        "question": "à¸œà¸´à¸§à¸«à¸™à¹‰à¸²à¹à¸«à¹‰à¸‡à¸¡à¸²à¸ à¸¡à¸µà¸£à¸´à¹‰à¸§à¸£à¸­à¸¢ à¸„à¸§à¸£à¸—à¸³à¸­à¸°à¹„à¸£à¸”à¸µà¸„à¸°",
        "category": "complex",
        "expected": ["MTS", "PDRN", "à¹à¸™à¸°à¸™à¸³"]
    },
    {
        "question": "à¸‡à¸š 10,000 à¸šà¸²à¸— à¸—à¸³à¸šà¸£à¸´à¸à¸²à¸£à¸­à¸°à¹„à¸£à¹„à¸”à¹‰à¸šà¹‰à¸²à¸‡",
        "category": "complex",
        "expected": ["à¸£à¸²à¸„à¸²", "à¸šà¸£à¸´à¸à¸²à¸£"]
    },
    
    # Edge Cases (1)
    {
        "question": "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸°",
        "category": "edge_case",
        "expected": ["à¸ªà¸§à¸±à¸ªà¸”à¸µ", "à¸¢à¸´à¸™à¸”à¸µ"]
    },
]


def quick_test():
    """Run quick test"""
    print("="*60)
    print("[START] Quick Model Test")
    print("="*60)
    
    # Check API key
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[ERROR] OPENAI_API_KEY not found in .env")
        print("ðŸ’¡ Please add your OpenAI API key to .env file:")
        print("   OPENAI_API_KEY=sk-your-key-here")
        return
    
    # Import service
    try:
        from core.enhanced_ai_service import get_enhanced_ai_service
    except ImportError as e:
        print(f"\n[ERROR] Import error: {e}")
        return
    
    # Initialize
    print("\nðŸ“¦ Initializing AI Service...")
    try:
        service = get_enhanced_ai_service(use_rag=True, use_vision=False)
        print("[OK] Service initialized")
    except Exception as e:
        print(f"[ERROR] Initialization error: {e}")
        return
    
    # Run tests
    print(f"\n[TEST] Running {len(QUICK_TEST_SAMPLES)} quick tests...\n")
    
    passed = 0
    failed = 0
    
    for i, test in enumerate(QUICK_TEST_SAMPLES, 1):
        question = test["question"]
        category = test["category"]
        expected = test["expected"]
        
        print(f"[{i}/{len(QUICK_TEST_SAMPLES)}] {category}")
        print(f"Q: {question}")
        
        try:
            # Query
            result = service.chat(message=question, use_cache=False)
            response = result.get("response", "")
            source = result.get("source", "unknown")
            latency = result.get("latency_ms", 0)
            
            # Check keywords
            response_lower = response.lower()
            found = [kw for kw in expected if kw.lower() in response_lower]
            
            # Status
            if len(found) >= len(expected) * 0.5:  # 50% threshold
                status = "[OK] PASS"
                passed += 1
            else:
                status = "[ERROR] FAIL"
                failed += 1
            
            print(f"A: {response[:150]}...")
            print(f"{status} | Source: {source} | Latency: {latency:.0f}ms")
            print(f"Keywords: {found}/{expected}")
            
        except Exception as e:
            print(f"[ERROR] ERROR: {e}")
            failed += 1
        
        print("-" * 60)
    
    # Summary
    total = len(QUICK_TEST_SAMPLES)
    pass_rate = passed / total * 100
    
    print("\n" + "="*60)
    print("[STATS] Quick Test Summary")
    print("="*60)
    print(f"Total: {total}")
    print(f"[OK] Passed: {passed} ({pass_rate:.1f}%)")
    print(f"[ERROR] Failed: {failed} ({100-pass_rate:.1f}%)")
    
    if pass_rate >= 80:
        print("\nðŸŽ‰ Model performing well!")
    elif pass_rate >= 60:
        print("\n[WARNING]  Model needs improvement")
    else:
        print("\n[ERROR] Model needs major fixes")
    
    print("\nðŸ’¡ Run full evaluation:")
    print("   python tests/evaluate_model.py")


if __name__ == "__main__":
    quick_test()
