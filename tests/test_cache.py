#!/usr/bin/env python3
"""Test cache performance"""

from core.ai_service import AIService
import time

print('='*80)
print('üî• CACHE PERFORMANCE TEST')
print('='*80)

service = AIService()

# Test same questions again - should be cached
questions = [
    'MTS PDRN ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∞',
    '‡∏°‡∏µ‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ä‡∏±‡πà‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏∞',
    '‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡∏Ñ‡∏∞',
]

print(f'\nüìä Initial Cache Stats:')
stats = service.get_cache_stats()
for key, val in stats.items():
    print(f'   ‚Ä¢ {key}: {val}')

print('\n' + '='*80)
print('Testing cached responses (should be < 50ms each):')
print('='*80)

for i, q in enumerate(questions, 1):
    start = time.time()
    response = service.chat_completion(
        [
            {'role': 'system', 'content': service.get_system_prompt()},
            {'role': 'user', 'content': q}
        ],
        stream=False,
        use_cache=True
    )
    latency = (time.time() - start) * 1000
    answer = ''.join(response)
    
    print(f'\n{i}. {q}')
    print(f'   ‚è±Ô∏è  Latency: {latency:.2f}ms')
    print(f'   üìè Length: {len(answer)} chars')
    if latency < 50:
        print(f'   ‚úÖ CACHED!')
    else:
        print(f'   ‚ö†Ô∏è  NOT CACHED (new request)')

print('\n' + '='*80)
print('üìä Final Cache Stats:')
print('='*80)
stats = service.get_cache_stats()
for key, val in stats.items():
    print(f'   ‚Ä¢ {key}: {val}')

print(f'\nüéØ Cache Hit Rate: {stats["hit_rate_percent"]}%')
if stats["hit_rate_percent"] > 80:
    print('‚úÖ Excellent cache performance!')
elif stats["hit_rate_percent"] > 50:
    print('‚úì Good cache performance')
else:
    print('‚ö†Ô∏è  Cache needs more data')

print('='*80)
