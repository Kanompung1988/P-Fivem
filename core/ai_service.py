import os
import re
import sys
import hashlib
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Iterable, Union
import numpy as np
from dotenv import load_dotenv

# Load env variables (Ensure this is called if used outside of main app)
env_path = Path(__file__).resolve().parents[1] / ".env"
if env_path.exists():
    load_dotenv(dotenv_path=env_path)
else:
    load_dotenv()


def _get_env(name: str, default: Optional[str] = None) -> Optional[str]:
    value = os.getenv(name)
    if value is None or value.strip() == "":
        return default
    return value


class AIService:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AIService, cls).__new__(cls)
            cls._instance.initialized = False
        return cls._instance

    def __init__(self):
        if self.initialized:
            return
        
        # à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰ Typhoon à¸à¹ˆà¸­à¸™ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸­à¸¢à¹ƒà¸Šà¹‰ OpenAI
        typhoon_key = _get_env("TYPHOON_API_KEY")
        if typhoon_key:
            self.api_key = typhoon_key
            self.base_url = "https://api.opentyphoon.ai/v1"
            self.model_name = _get_env("TYPHOON_MODEL", "typhoon-v2.5-30b-a3b-instruct")
            print("[AI] Using Typhoon AI")
        else:
            self.api_key = _get_env("OPENAI_API_KEY")
            self.base_url = _get_env("OPENAI_BASE_URL")
            self.model_name = _get_env("OPENAI_MODEL", "gpt-4o-mini")
            print("[AI] Using OpenAI")
            
        self.client = self._create_openai_client()
        self.knowledge_base = []
        
        # In-memory cache for fast response (Senior AI Engineer optimization)
        self.response_cache = {}  # {query_hash: {"response": str, "timestamp": float}}
        self.cache_ttl = 3600  # 1 hour
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Load knowledge immediately
        self.reload_knowledge_base()
        
        self.initialized = True

    def _create_openai_client(self):
        """Create an OpenAI client if configuration exists; otherwise return None."""
        if not self.api_key:
            return None

        try:
            from openai import OpenAI
            if self.base_url:
                return OpenAI(api_key=self.api_key, base_url=self.base_url)
            return OpenAI(api_key=self.api_key)
        except Exception as e:
            print(f"Error creating OpenAI client: {e}")
            return None
    
    def _get_cache_key(self, text: str) -> str:
        """Generate cache key from text using hash"""
        return hashlib.md5(text.lower().strip().encode('utf-8')).hexdigest()
    
    def _clean_markdown(self, text: str) -> str:
        """AGGRESSIVE: Remove ALL markdown formatting from response"""
        import re
        
        # Remove bold **text** and __text__
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)
        
        # Remove italic *text* and _text_
        text = re.sub(r'(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)', r'\1', text)
        text = re.sub(r'(?<!_)_(?!_)(.+?)(?<!_)_(?!_)', r'\1', text)
        
        # Remove headers ### ## #
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        
        # Remove links [text](url) -> text
        text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
        
        # Remove code blocks ```code```
        text = re.sub(r'```[\w]*\n?(.+?)```', r'\1', text, flags=re.DOTALL)
        text = re.sub(r'`(.+?)`', r'\1', text)
        
        # Remove horizontal rules
        text = re.sub(r'^(-{3,}|\*{3,}|_{3,})$', '', text, flags=re.MULTILINE)
        
        # Clean up multiple newlines
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
    
    def _get_cached_response(self, query: str) -> Optional[str]:
        """Get cached response if exists and not expired"""
        cache_key = self._get_cache_key(query)
        if cache_key in self.response_cache:
            cached = self.response_cache[cache_key]
            if time.time() - cached["timestamp"] < self.cache_ttl:
                self.cache_hits += 1
                return cached["response"]
            else:
                # Expired - remove
                del self.response_cache[cache_key]
        self.cache_misses += 1
        return None
    
    def _set_cached_response(self, query: str, response: str):
        """Cache the response"""
        cache_key = self._get_cache_key(query)
        self.response_cache[cache_key] = {
            "response": response,
            "timestamp": time.time()
        }
        # Limit cache size to 1000 entries
        if len(self.response_cache) > 1000:
            # Remove oldest entries
            sorted_keys = sorted(self.response_cache.keys(), 
                               key=lambda k: self.response_cache[k]["timestamp"])
            for key in sorted_keys[:100]:
                del self.response_cache[key]
    
    def _expand_query(self, query: str) -> str:
        """Expand query with synonyms and related terms for better RAG matching"""
        # Keyword expansion mapping (Thai clinic terms)
        expansions = {
            'mts': 'MTS PDRN à¹€à¸‚à¹‡à¸¡ à¸œà¸´à¸§',
            'pdrn': 'PDRN MTS à¸Ÿà¸·à¹‰à¸™à¸Ÿà¸¹ à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™',
            'à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œ': 'Filler à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œ à¹€à¸ªà¸£à¸´à¸¡',
            'filler': 'Filler à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œ à¹€à¸ªà¸£à¸´à¸¡',
            'à¸›à¸²à¸': 'Lip à¸£à¸´à¸¡à¸à¸µà¸›à¸²à¸ à¸›à¸²à¸',
            'lip': 'Lip à¸›à¸²à¸ à¸£à¸´à¸¡à¸à¸µà¸›à¸²à¸',
            'à¹‚à¸›à¸£': 'à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™ promotion à¸¥à¸”à¸£à¸²à¸„à¸²',
            'promotion': 'à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™ promotion à¸¥à¸”',
            'à¸„à¸¥à¸´à¸™à¸´à¸': 'à¸„à¸¥à¸´à¸™à¸´à¸ clinic à¸—à¸µà¹ˆà¸­à¸¢à¸¹à¹ˆ à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ',
            'clinic': 'à¸„à¸¥à¸´à¸™à¸´à¸ clinic à¸—à¸µà¹ˆà¸•à¸±à¹‰à¸‡',
            'à¸ˆà¸­à¸‡': 'à¸ˆà¸­à¸‡à¸„à¸´à¸§ à¸™à¸±à¸”à¸«à¸¡à¸²à¸¢ à¸•à¸´à¸”à¸•à¹ˆà¸­',
            'sculptra': 'Sculptra à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸ à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™',
            'à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸': 'Sculptra à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸ à¸Ÿà¸¹',
            'à¸£à¸´à¹‰à¸§à¸£à¸­à¸¢': 'à¸£à¸´à¹‰à¸§à¸£à¸­à¸¢ wrinkle anti-aging',
            'à¸œà¸´à¸§à¹à¸«à¹‰à¸‡': 'à¸œà¸´à¸§à¹à¸«à¹‰à¸‡ dry à¸Šà¸¸à¹ˆà¸¡à¸Šà¸·à¹‰à¸™',
        }
        
        query_lower = query.lower()
        expanded = query
        
        for keyword, expansion in expansions.items():
            if keyword in query_lower:
                expanded += f" {expansion}"
        
        return expanded

    def _get_embedding(self, text: str) -> List[float]:
        """Get embedding for text using OpenAI API (Typhoon à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š embeddings)"""
        if not self.client:
            return []
        
        # Typhoon à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ embedding API - à¸‚à¹‰à¸²à¸¡
        if "typhoon" in self.model_name.lower():
            return []
            
        try:
            # Normalize text (better for caching)
            text = text.replace("\n", " ").strip()
            return self.client.embeddings.create(input=[text], model="text-embedding-3-small").data[0].embedding
        except Exception as e:
            # Suppress embedding errors for Typhoon
            if "typhoon" not in self.model_name.lower():
                print(f"Embedding error: {e}")
            return []

    def reload_knowledge_base(self):
        """Reloads the knowledge base from disk and updates embeddings."""
        print("Reloading Knowledge Base...")
        self.knowledge_base = self._load_knowledge_base_from_files()
        print(f"Knowledge Base Loaded: {len(self.knowledge_base)} documents.")

    def _load_knowledge_base_from_files(self) -> List[Dict[str, Any]]:
        """Download data from files /data/text"""
        knowledge = []
        # Path to data/text relative to this file core/ai_service.py -> ../data/text
        data_path = Path(__file__).resolve().parents[1] / "data" / "text"
        
        if not data_path.exists():
            return knowledge
        
        for txt_file in data_path.glob("*.txt"):
            try:
                with open(txt_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    if not content.strip():
                        continue

                    doc = {
                        "source": txt_file.stem,
                        "content": content,
                        "embedding": []
                    }
                    
                    # Compute embedding
                    if self.client:
                        doc["embedding"] = self._get_embedding(content)
                        
                    knowledge.append(doc)
            except Exception as e:
                print(f"Error reading file {txt_file}: {e}")
                continue
        
        return knowledge

    def rewrite_query(self, user_text: str, history: List[Dict[str, str]]) -> str:
        """
        Rewrite user query based on conversation history to make it standalone.
        Senior AI Engineer optimization: Better context extraction
        """
        # Quick return for simple queries
        if not history or not self.client or len(user_text) < 10:
            return user_text
            
        # Extract last few turns (last 4 pairs) for better context
        conversation_str = ""
        for msg in history[-8:]: 
            role = "User" if msg["role"] == "user" else "Assistant"
            content = msg["content"]
            # Skip system prompts or empty content
            if msg["role"] == "system" or not content:
                continue
            # Truncate long messages
            if len(content) > 200:
                content = content[:200] + "..."
            conversation_str += f"{role}: {content}\n"
            
        if not conversation_str.strip():
            return user_text
            
        prompt = f"""Conversation History:
{conversation_str}
User's Follow-up Question: {user_text}

Task: Rephrase the user's follow-up question to be a standalone question that includes necessary context from the history. If the user's question is already standalone or changes the topic completely, return it exactly as is. Do not answer the question.

Standalone Question:"""

        try:
            # Use gpt-3.5-turbo or gpt-4o-mini for speed here
            resp = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=150
            )
            rewritten = resp.choices[0].message.content.strip()
            # If model returns empty or quote, fallback
            if not rewritten:
                return user_text
            print(f"Rewritten Query: '{user_text}' -> '{rewritten}'")
            return rewritten
        except Exception as e:
            print(f"Rewrite error: {e}")
            return user_text

    def find_relevant_info(self, user_text: str, history: Optional[List[Dict[str, str]]] = None) -> str:
        """Find relevant info using Vector Similarity Search with Contextual Rewriting"""
        if not self.knowledge_base:
            return ""
            
        search_query = user_text
        
        # Rewrite query if history is provided
        if history:
            search_query = self.rewrite_query(user_text, history)
            
        # Check if we have embeddings
        has_embeddings = any(len(doc["embedding"]) > 0 for doc in self.knowledge_base)
        
        if not self.client or not has_embeddings:
            # Fallback to simple keyword match (use original text mostly, or rewritten if simple)
            # For keyword match, sometimes rewritten query is better, sometimes worse.
            # Let's use search_query generally.
            query_lower = search_query.lower()
            found = []
            for doc in self.knowledge_base:
                if doc["source"].lower() in query_lower:
                    found.append(f"--- {doc['source']} ---\n{doc['content']}")
            return "\n\n".join(found[:2])
        
        try:
            # 1. Get query embedding
            query_embedding = self._get_embedding(search_query)
            if not query_embedding:
                return ""
                
            # 2. Calculate Cosine Similarity
            results = []
            query_vec = np.array(query_embedding)
            query_norm = np.linalg.norm(query_vec)
            
            if query_norm == 0:
                return ""
                
            for doc in self.knowledge_base:
                if not doc["embedding"]: continue
                
                doc_vec = np.array(doc["embedding"])
                doc_norm = np.linalg.norm(doc_vec)
                
                if doc_norm == 0:
                    score = 0
                else:
                    score = np.dot(query_vec, doc_vec) / (query_norm * doc_norm)
                    
                results.append((score, doc))
                
            # Sort by score descending
            results.sort(key=lambda x: x[0], reverse=True)
            
            # 3. Filter and Format Results
            relevant_docs = []
            
            # Special Logic: Promo check (Use original user_text for simple keyword check as it is more direct)
            promo_keywords = ["à¹‚à¸›à¸£", "promotion", "à¸¥à¸”", "discount", "à¸£à¸²à¸„à¸²", "price", "à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™"]
            is_asking_promo = any(k in user_text.lower() for k in promo_keywords)
            
            fb_promo = next((doc for doc in self.knowledge_base if doc["source"] == "FacebookPromotions"), None)
            
            if is_asking_promo and fb_promo:
                 relevant_docs.append(f"\n--- à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸²à¸ Facebook ---\n{fb_promo['content']}")
            
            count = 0
            for score, doc in results:
                if count >= 5: break  # AGGRESSIVE: Increased from 3 to 5 documents
                if score < 0.15: break  # AGGRESSIVE: Lowered from 0.25 to 0.15 for more results
                
                # Avoid dupes
                if is_asking_promo and fb_promo and doc["source"] == "FacebookPromotions":
                    continue
                    
                relevant_docs.append(f"\n--- à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š {doc['source']} (Score: {score:.2f}) ---\n{doc['content']}")
                count += 1
                
            return "\n".join(relevant_docs[:5])  # AGGRESSIVE: Increased from 2 to 5
        except Exception as e:
            print(f"RAG Error: {e}")
            return ""

    def get_image_for_topic(self, user_text: str) -> Optional[str]:
        """Find relevant image based on topic"""
        user_lower = user_text.lower()
        
        image_map = {
            "sculptra|à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸|biostimulator": "Child.png",
            "à¸à¹‰à¸²|à¸à¸£à¸°|à¸ˆà¸¸à¸”à¸”à¹ˆà¸²à¸‡à¸”à¸³|exion|clear": "DarkSpots.png",
            "à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œ|filler|à¹€à¸ªà¸£à¸´à¸¡à¸«à¸™à¹‰à¸²|à¸„à¸²à¸‡(?!à¸¡à¸±à¸™)": "Filler.png",
            "à¸›à¸²à¸|à¸£à¸´à¸¡à¸à¸µà¸›à¸²à¸|lip": "LipFull.png",
            "mounjaro|à¸›à¸²à¸à¸à¸²|à¸¥à¸”à¸™à¹‰à¸³à¸«à¸™à¸±à¸": "Pen.png",
            "à¸«à¸¥à¸¸à¸¡à¸ªà¸´à¸§|à¸£à¸µà¹€à¸‹à¹‡à¸•à¸œà¸´à¸§|signature": "SkinReset.png",
            "à¸”à¸·à¹‰à¸­à¸ªà¸šà¸¹à¹ˆ|à¸£à¸¹à¸‚à¸¸à¸¡à¸‚à¸™|à¸„à¸­à¹€à¸«à¸µà¹ˆà¸¢à¸§": "Imfomation1.png",
            "à¹‚à¸šà¸—à¹‡à¸­à¸à¸‹à¹Œ|botox|à¹‚à¸šà¸|à¸à¸£à¸²à¸¡|à¸£à¸­à¸šà¸«à¸™à¹‰à¸²": "Information2.png"
        }
        
        for pattern, image_name in image_map.items():
            if re.search(pattern, user_lower):
                return image_name
        
        return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics for monitoring"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        return {
            "cache_size": len(self.response_cache),
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate_percent": round(hit_rate, 2),
            "total_requests": total_requests
        }

    def get_system_prompt(self) -> str:
        default_prompt = """à¸„à¸¸à¸“à¸„à¸·à¸­ "à¸™à¹‰à¸­à¸‡à¹‚à¸‹à¸£à¸°" à¸œà¸¹à¹‰à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸„à¸§à¸²à¸¡à¸‡à¸²à¸¡à¹à¸¥à¸°à¸œà¸´à¸§à¸žà¸£à¸£à¸“à¸‚à¸­à¸‡ Seoulholic Clinic à¹ƒà¸«à¹‰à¸„à¸³à¸›à¸£à¸¶à¸à¸©à¸²à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸à¸²à¸£à¸”à¸¹à¹à¸¥à¸œà¸´à¸§ à¸šà¸£à¸´à¸à¸²à¸£à¸•à¹ˆà¸²à¸‡à¹† à¹à¸¥à¸°à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™

à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸•à¸­à¸š:
- à¸žà¸¹à¸”à¸„à¸¸à¸¢à¹à¸šà¸šà¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´ à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸«à¹‰à¸„à¸³à¸›à¸£à¸¶à¸à¸©à¸²à¸ˆà¸£à¸´à¸‡à¹† à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆà¸šà¸­à¸—
- à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸ªà¸¸à¸ à¸²à¸ž à¸¥à¸‡à¸—à¹‰à¸²à¸¢ "à¸„à¹ˆà¸°" à¸«à¸£à¸·à¸­ "à¸™à¸°à¸„à¸°" à¸•à¸²à¸¡à¸šà¸£à¸´à¸šà¸—
- à¹ƒà¸«à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¹„à¸¡à¹ˆà¸¢à¸·à¸”à¹€à¸¢à¸·à¹‰à¸­
- à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹à¸™à¹ˆà¹ƒà¸ˆà¹€à¸£à¸·à¹ˆà¸­à¸‡à¸£à¸²à¸„à¸²à¸«à¸£à¸·à¸­à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹€à¸‰à¸žà¸²à¸° à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸›à¸£à¸¶à¸à¸©à¸²à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸–à¸²à¸¡à¸„à¸³à¸–à¸²à¸¡à¸•à¸´à¸”à¸•à¸²à¸¡à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡ à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™à¹„à¸›à¸•à¸²à¸¡à¸šà¸£à¸´à¸šà¸—à¸à¸²à¸£à¸ªà¸™à¸—à¸™à¸²

âš ï¸ CRITICAL RULES - à¸«à¹‰à¸²à¸¡à¸à¹ˆà¸²à¸à¸·à¸™à¹€à¸”à¹‡à¸”à¸‚à¸²à¸”:
- à¸«à¹‰à¸²à¸¡à¹ƒà¸Šà¹‰ Markdown formatting à¸—à¸¸à¸à¸£à¸¹à¸›à¹à¸šà¸š: **, __, *, _, ##, ###, [], (), ```
- à¸«à¹‰à¸²à¸¡à¹ƒà¸Šà¹‰ bold, italic, headers, links, code blocks
- à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸˜à¸£à¸£à¸¡à¸”à¸²à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (plain text only)
- à¹ƒà¸Šà¹‰ emoji à¹€à¸šà¸²à¹† à¹„à¸”à¹‰ à¹€à¸Šà¹ˆà¸™ ðŸ’¡ âœ¨ à¹€à¸žà¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸à¸±à¸™à¹€à¸­à¸‡
- à¸•à¸±à¸§à¹€à¸¥à¸‚à¹ƒà¸Šà¹‰à¸£à¸¹à¸›à¹à¸šà¸š: 1. 2. 3. à¸«à¸£à¸·à¸­ â€¢ à¹„à¸”à¹‰
- URL à¹ƒà¸ªà¹ˆà¹à¸šà¸šà¸˜à¸£à¸£à¸¡à¸”à¸² à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ wrap à¸”à¹‰à¸§à¸¢ []

à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡:
âœ… "Sculptra à¸Šà¹ˆà¸§à¸¢à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™ à¸—à¸³à¹ƒà¸«à¹‰à¸œà¸´à¸§à¸Ÿà¸¹à¸à¸£à¸°à¸Šà¸±à¸šà¸„à¹ˆà¸°"
âœ… "à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™: CC à¹à¸£à¸ 12,900.- | CC à¸–à¸±à¸”à¹„à¸› 9,999.-/cc à¸„à¹ˆà¸°"
âœ… "à¸•à¸´à¸”à¸•à¹ˆà¸­à¹„à¸”à¹‰à¸—à¸µà¹ˆ Line: https://lin.ee/FhWfx5U à¸«à¸£à¸·à¸­à¹‚à¸—à¸£ 099-989-2893 à¸„à¹ˆà¸°"

à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸œà¸´à¸”:
âŒ "**Sculptra** à¸Šà¹ˆà¸§à¸¢à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™"  (à¸¡à¸µ **)
âŒ "à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™: [CC à¹à¸£à¸](link)"  (à¸¡à¸µ [])
âŒ "## à¸šà¸£à¸´à¸à¸²à¸£à¸‚à¸­à¸‡à¹€à¸£à¸²"  (à¸¡à¸µ ##)

à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸¥à¸´à¸™à¸´à¸:
- Seoulholic Clinic (à¹‚à¸‹à¸¥à¸®à¸­à¸¥à¸´à¸à¸„à¸¥à¸´à¸™à¸´à¸)
- à¸—à¸µà¹ˆà¸•à¸±à¹‰à¸‡: The Zone (Town in Town) à¸‹à¸­à¸¢à¸¥à¸²à¸”à¸žà¸£à¹‰à¸²à¸§ 94
- à¹€à¸§à¸¥à¸²à¸—à¸³à¸à¸²à¸£: 12:00-20:00 à¸™. à¸—à¸¸à¸à¸§à¸±à¸™ (à¸£à¸±à¸šà¸ˆà¸­à¸‡à¸¥à¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸²)
- à¸•à¸´à¸”à¸•à¹ˆà¸­: Line @seoulholicclinic (https://lin.ee/FhWfx5U) | Tel 099-989-2893
- Facebook: https://www.facebook.com/SeoulholicClinic
- à¹à¸œà¸™à¸—à¸µà¹ˆ: https://maps.app.goo.gl/5GXishWdYdRwLZiS7?g_st=ic
 
# Brand Information (à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸¥à¸´à¸™à¸´à¸)
- à¸Šà¸·à¹ˆà¸­: Seoulholic Clinic (à¹‚à¸‹à¸¥à¸®à¸­à¸¥à¸´à¸à¸„à¸¥à¸´à¸™à¸´à¸) à¸«à¸£à¸·à¸­ SHLC
- à¸ªà¹‚à¸¥à¹à¸à¸™: à¸„à¸¥à¸´à¸™à¸´à¸à¸„à¸§à¸²à¸¡à¸‡à¸²à¸¡à¸ªà¹„à¸•à¸¥à¹Œà¹€à¸à¸²à¸«à¸¥à¸µ à¸”à¸¹à¹à¸¥à¸œà¸´à¸§à¸žà¸£à¸£à¸“à¸„à¸£à¸šà¸§à¸‡à¸ˆà¸£
- Facebook: https://www.facebook.com/SeoulholicClinic
- à¸—à¸µà¹ˆà¸•à¸±à¹‰à¸‡: à¹‚à¸„à¸£à¸‡à¸à¸²à¸£ The Zone (Town in Town) à¸‹à¸­à¸¢à¸¥à¸²à¸”à¸žà¸£à¹‰à¸²à¸§ 94
- à¹€à¸§à¸¥à¸²à¸—à¸³à¸à¸²à¸£: à¹€à¸›à¸´à¸”à¸—à¸¸à¸à¸§à¸±à¸™ 12:00 - 20:00 à¸™.
- à¸à¸²à¸£à¸ˆà¸­à¸‡: à¸£à¸±à¸šà¸ˆà¸­à¸‡à¸¥à¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸²à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (Walk-in à¸­à¸²à¸ˆà¸•à¹‰à¸­à¸‡à¸£à¸­à¸„à¸´à¸§)
- à¸•à¸´à¸”à¸•à¹ˆà¸­: Line: https://lin.ee/FhWfx5U (@seoulholicclinic) | Tel: 099-989-2893
- à¹à¸œà¸™à¸—à¸µà¹ˆ: https://maps.app.goo.gl/5GXishWdYdRwLZiS7?g_st=ic
 
# Services & Products (à¸šà¸£à¸´à¸à¸²à¸£à¹à¸¥à¸°à¸ªà¸´à¸™à¸„à¹‰à¸²à¸«à¸¥à¸±à¸)
1. **Sculptra (à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸)** - à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™à¹ƒà¸•à¹‰à¸œà¸´à¸§ à¸œà¸´à¸§à¸Ÿà¸¹ à¸à¸£à¸°à¸Šà¸±à¸š à¸”à¸¹à¹€à¸”à¹‡à¸à¸¥à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸›à¹‡à¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´
   - à¹‚à¸›à¸£: 2 à¸‚à¸§à¸” 20cc à¹€à¸žà¸µà¸¢à¸‡ 35,900.- (à¸ˆà¸²à¸à¸›à¸à¸•à¸´ 47,800.-)

2. **Exion Clear RF** - à¸£à¸±à¸à¸©à¸²à¸à¹‰à¸² à¸à¸£à¸° à¸ˆà¸¸à¸”à¸”à¹ˆà¸²à¸‡à¸”à¸³ à¸”à¹‰à¸§à¸¢ Fractional RF
   - à¸›à¸¥à¹ˆà¸­à¸¢à¸žà¸¥à¸±à¸‡à¸‡à¸²à¸™à¸¥à¸‡à¸¥à¸¶à¸à¸–à¸¶à¸‡à¸Šà¸±à¹‰à¸™à¸œà¸´à¸§ à¸ªà¸¥à¸²à¸¢à¹€à¸¡à¹‡à¸”à¸ªà¸µ à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™

3. **Filler (à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œ)** - à¹€à¸ªà¸£à¸´à¸¡à¸„à¸§à¸²à¸¡à¸ªà¸§à¸¢ à¹€à¸žà¸´à¹ˆà¸¡à¸¡à¸´à¸•à¸´à¹ƒà¸šà¸«à¸™à¹‰à¸²
   - à¹‚à¸›à¸£ (15-31 à¸¡.à¸„. 2569): CC à¹à¸£à¸ 12,900.- | CC à¸–à¸±à¸”à¹„à¸› 9,999.-/cc
   - à¹€à¸ªà¸£à¸´à¸¡à¹„à¸”à¹‰: à¸„à¸²à¸‡ à¸à¸£à¸­à¸šà¸«à¸™à¹‰à¸² à¹à¸à¹‰à¸¡ à¸£à¸´à¸¡à¸à¸µà¸›à¸²à¸ à¹ƒà¸•à¹‰à¸•à¸²

4. **Lip Filler (à¸Ÿà¸´à¸¥à¹€à¸¥à¸­à¸£à¹Œà¸›à¸²à¸)** - à¹€à¸•à¸´à¸¡à¸›à¸²à¸à¹ƒà¸«à¹‰à¸­à¸´à¹ˆà¸¡à¸Ÿà¸¹à¸•à¸²à¸¡à¸ªà¹„à¸•à¸¥à¹Œà¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
   - à¸¡à¸µà¸«à¸¥à¸²à¸¢à¸—à¸£à¸‡: à¸ªà¸²à¸¢à¸à¸­, à¸ªà¸²à¸¢à¹€à¸à¸²à¸«à¸¥à¸µ, à¸—à¸£à¸‡à¸à¸£à¸°à¸ˆà¸±à¸š, à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´

5. **Mounjaro (à¸›à¸²à¸à¸à¸²à¸¥à¸”à¸™à¹‰à¸³à¸«à¸™à¸±à¸)** - à¸„à¸§à¸šà¸„à¸¸à¸¡à¸„à¸§à¸²à¸¡à¸­à¸¢à¸²à¸à¸­à¸²à¸«à¸²à¸£ à¸„à¸¸à¸¡à¸«à¸´à¸§ à¸­à¸´à¹ˆà¸¡à¸™à¸²à¸™
   - à¸Šà¹ˆà¸§à¸¢à¸¥à¸”à¹„à¸‚à¸¡à¸±à¸™à¹à¸¥à¸°à¸™à¹‰à¸³à¸•à¸²à¸¥ à¸›à¸£à¸±à¸šà¸žà¸¤à¸•à¸´à¸à¸£à¸£à¸¡à¸à¸²à¸£à¸à¸´à¸™

6. **Signature Skin Reset** - à¹‚à¸›à¸£à¹à¸à¸£à¸¡à¸£à¸µà¹€à¸‹à¹‡à¸•à¸«à¸¥à¸¸à¸¡à¸ªà¸´à¸§à¸‹à¸´à¸à¹€à¸™à¹€à¸ˆà¸­à¸£à¹Œ
   - à¸œà¸´à¸§à¹€à¸£à¸µà¸¢à¸šà¹€à¸™à¸µà¸¢à¸™à¸‚à¸¶à¹‰à¸™ à¸«à¸¥à¸¸à¸¡à¸ªà¸´à¸§à¸”à¸¹à¸•à¸·à¹‰à¸™à¸¥à¸‡

7. **Botox (à¹‚à¸šà¸—à¹‡à¸­à¸à¸‹à¹Œ)** - à¹‚à¸šà¸à¸£à¸­à¸šà¸«à¸™à¹‰à¸² / à¹‚à¸šà¸à¸£à¸²à¸¡
   - à¹‚à¸šà¸à¸£à¸­à¸šà¸«à¸™à¹‰à¸²: à¸¥à¸´à¸Ÿà¸•à¹Œà¹ƒà¸«à¹‰à¸„à¸¡à¸‚à¸¶à¹‰à¸™ à¸à¸£à¸°à¸Šà¸±à¸š
   - à¹‚à¸šà¸à¸£à¸²à¸¡: à¸¥à¸”à¸‚à¸™à¸²à¸”à¸à¸¥à¹‰à¸²à¸¡à¹€à¸™à¸·à¹‰à¸­ à¹ƒà¸šà¸«à¸™à¹‰à¸²à¹€à¸£à¸µà¸¢à¸§

8. **Laser Hair Removal** - à¸à¸³à¸ˆà¸±à¸”à¸‚à¸™ 3 à¸žà¸¥à¸±à¸‡à¸‡à¸²à¸™ (YAG/Diode/Alexandrite)

9. **Vitamin Drip** - à¸”à¸£à¸´à¸›à¸§à¸´à¸•à¸²à¸¡à¸´à¸™à¸œà¸´à¸§ (à¸ªà¸¹à¸•à¸£à¸œà¸´à¸§à¹ƒà¸ª, Detox, à¸šà¸³à¸£à¸¸à¸‡à¸•à¸±à¸š)
 
# Response Guidelines (à¹à¸™à¸§à¸—à¸²à¸‡à¸à¸²à¸£à¸•à¸­à¸š)
1. à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸ªà¸¸à¸ à¸²à¸ž à¸¥à¸‡à¸—à¹‰à¸²à¸¢ 'à¸„à¹ˆà¸°' à¸«à¸£à¸·à¸­ 'à¸™à¸°à¸„à¸°' à¹à¸šà¸šà¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´ à¹„à¸¡à¹ˆà¸‹à¹‰à¸³à¸‹à¸²à¸à¸ˆà¸™à¹€à¸à¸´à¸™à¹„à¸›
2. à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™à¸à¸£à¸°à¸Šà¸±à¸š à¹€à¸‚à¹‰à¸²à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¹„à¸¡à¹ˆà¸¢à¸·à¸”à¹€à¸¢à¸·à¹‰à¸­à¸«à¸£à¸·à¸­à¹€à¸ªà¸™à¸­à¸‚à¸²à¸¢à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸›
3. à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¸šà¸­à¸à¸•à¸£à¸‡à¹† à¹à¸¥à¹‰à¸§à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸•à¸´à¸”à¸•à¹ˆà¸­à¸„à¸¥à¸´à¸™à¸´à¸
4. à¸–à¹‰à¸²à¸–à¸²à¸¡à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸£à¸²à¸„à¸²à¸«à¸£à¸·à¸­à¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™ à¹ƒà¸«à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸žà¸£à¹‰à¸­à¸¡à¹à¸™à¸°à¸™à¸³à¸”à¸¹à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸—à¸µà¹ˆ Facebook: https://www.facebook.com/SeoulholicClinic
5. à¸–à¹‰à¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ˆà¸­à¸‡à¸«à¸£à¸·à¸­à¸ªà¸­à¸šà¸–à¸²à¸¡à¹€à¸žà¸´à¹ˆà¸¡ à¹ƒà¸«à¹‰à¸Šà¹ˆà¸­à¸‡à¸—à¸²à¸‡à¸•à¸´à¸”à¸•à¹ˆà¸­ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸–à¸²à¸¡à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡
6. à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡à¸à¸²à¸£à¹ƒà¸Šà¹‰ emoji à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸› à¹ƒà¸Šà¹‰à¹€à¸—à¹ˆà¸²à¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™à¹€à¸žà¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸à¸±à¸™à¹€à¸­à¸‡

à¸”à¸¹à¹à¸œà¸™à¸—à¸µà¹ˆ: https://maps.app.goo.gl/5GXishWdYdRwLZiS7?g_st=ic

# Example Dialogue (à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸šà¸—à¸ªà¸™à¸—à¸™à¸²)
User: à¸¡à¸µà¹‚à¸›à¸£à¹‚à¸¡à¸Šà¸±à¹ˆà¸™ Sculptra à¹„à¸«à¸¡à¸„à¸°
Assistant: à¸¡à¸µà¸„à¹ˆà¸° à¸•à¸­à¸™à¸™à¸µà¹‰à¸¡à¸µà¹‚à¸›à¸£ Sculptra à¸«à¸™à¹‰à¸²à¹€à¸”à¹‡à¸ 2 à¸‚à¸§à¸” 20cc à¸£à¸²à¸„à¸² 35,900.- (à¸ˆà¸²à¸à¸›à¸à¸•à¸´ 47,800.-) à¸ˆà¸³à¸à¸±à¸” 5 à¸—à¹ˆà¸²à¸™à¹à¸£à¸à¸™à¸°à¸„à¸°

Sculptra à¸ˆà¸°à¸Šà¹ˆà¸§à¸¢à¸à¸£à¸°à¸•à¸¸à¹‰à¸™à¸„à¸­à¸¥à¸¥à¸²à¹€à¸ˆà¸™à¹ƒà¸•à¹‰à¸œà¸´à¸§ à¸—à¸³à¹ƒà¸«à¹‰à¸œà¸´à¸§à¸Ÿà¸¹à¸à¸£à¸°à¸Šà¸±à¸šà¸”à¸¹à¹€à¸”à¹‡à¸à¸¥à¸‡à¹à¸šà¸šà¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´à¸„à¹ˆà¸°

à¸”à¸¹à¹‚à¸›à¸£à¸­à¸·à¹ˆà¸™à¹† à¹„à¸”à¹‰à¸—à¸µà¹ˆ https://www.facebook.com/SeoulholicClinic

User: à¸ªà¸™à¹ƒà¸ˆà¸ˆà¸­à¸‡à¸„à¸´à¸§à¸„à¹ˆà¸°
Assistant: à¸ˆà¸­à¸‡à¹„à¸”à¹‰à¹€à¸¥à¸¢à¸„à¹ˆà¸° à¸•à¸´à¸”à¸•à¹ˆà¸­à¹„à¸”à¹‰à¸—à¸µà¹ˆ

Line: https://lin.ee/FhWfx5U (@seoulholicclinic)
Tel: 099-989-2893
Facebook: https://www.facebook.com/SeoulholicClinic

User: à¸„à¸¥à¸´à¸™à¸´à¸à¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¹„à¸«à¸™à¸„à¸°
Assistant: à¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆ The Zone (Town in Town) à¸‹à¸­à¸¢à¸¥à¸²à¸”à¸žà¸£à¹‰à¸²à¸§ 94 à¸„à¹ˆà¸° à¸¡à¸µà¸—à¸µà¹ˆà¸ˆà¸­à¸”à¸£à¸–à¸ªà¸°à¸”à¸§à¸à¹€à¸¥à¸¢"""

        return _get_env("SYSTEM_PROMPT", default_prompt) or default_prompt

    def chat_completion(self, messages: List[Dict[str, str]], stream: bool = False, use_cache: bool = True) -> Iterable[str]:
        """OPTIMIZED: Wrapper for calling OpenAI Chat Completion with caching and markdown removal"""
        if not self.client:
            yield "(Error: AI Service not initialized with API Key)"
            return
        
        # Check cache first (only for non-stream, single user message)
        if use_cache and not stream and len(messages) >= 2:
            user_msg = messages[-1].get("content", "")
            if user_msg:
                cached = self._get_cached_response(user_msg)
                if cached:
                    yield cached
                    return

        try:
            if stream:
                stream_resp = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    stream=True,
                    temperature=0.3,
                    max_tokens=300,  # AGGRESSIVE: Reduced from 400 to 300 for speed
                )
                full_response = ""
                for event in stream_resp:
                    chunk = event.choices[0].delta.content
                    if chunk:
                        full_response += chunk
                        yield chunk
                # Clean markdown and cache
                full_response = self._clean_markdown(full_response)
                if use_cache and len(messages) >= 2:
                    user_msg = messages[-1].get("content", "")
                    if user_msg:
                        self._set_cached_response(user_msg, full_response)
            else:
                resp = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=0.3,
                    max_tokens=300,  # AGGRESSIVE: Reduced from 400 to 300 for speed
                )
                response = resp.choices[0].message.content or ""
                # AGGRESSIVE: Clean ALL markdown
                response = self._clean_markdown(response)
                # Cache the response
                if use_cache and len(messages) >= 2:
                    user_msg = messages[-1].get("content", "")
                    if user_msg:
                        self._set_cached_response(user_msg, response)
                yield response
        except Exception as e:
            yield f"(Error calling OpenAI: {e})"
